---
# You don't need to edit this file, it's empty on purpose.
# Edit theme's home layout instead if you wanna make some changes
# See: https://jekyllrb.com/docs/themes/#overriding-theme-defaults
layout: single
title: The BERT for Humanists Project
---

The BERT for Humanists Project seeks to develop resources for digital humanists to explore if and how BERT-like models can be used in their research and teaching. 

<img width=400 src="https://www.neh.gov/sites/default/files/inline-files/NEH-Preferred-Seal-Transparent820.png"/> <img width=200 src="assets/images/cornell_seal_simple_white.svg"/>

## What is BERT?

Developed by Google and released openly to the public in late 2018, [BERT](https://ai.googleblog.com/2018/11/open-sourcing-bert-state-of-art-pre.html) is a state-of-the-art NLP method trained on a very large dataset of texts—namely, the entirety of English-language Wikipedia (2,500 million words) and a corpus of English-language books (800 million words). Thanks to this large amount of training data and its unique neural network architecture, BERT—--and subsequent methods like it (e.g., [GPT-2]([https://openai.com/blog/better-language-models/))--—can understand human language significantly better than previous NLP methods.

For example, BERT can identify whether a sentence expresses positive or negative sentiment, predict what sentence should come next in a paragraph, and disambiguate between multivalent words with never-before-seen levels of accuracy.

What impact might BERT-like models have in the field of the digital humanities? What impact might digital humanists have in turn on our understanding and application of BERT-like methods? 

