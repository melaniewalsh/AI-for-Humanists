---
permalink: /bibliography/
title: "Annotated Bibliography"
toc_label: " "
toc_icon: "star"
toc_sticky: true
toc: true
---

This is a crowdsourced annotated bibliography of research and resources related to BERT-like models. 

If you'd like to add to the bibliography, you can do so in [this Dropbox document](https://www.dropbox.com/scl/fi/w9w2bs55o0fm1upl2hrhz/BERT-for-Humanists-Annotated-Bibliography.paper?dl=0&rlkey=7qtjce0tilgg42sn7kywwqloh). We will update the bibliography on this web page periodically.

# Technical Readings

- [**‚ÄúBERT: Pre-Training of Deep Bidirectional Transformers for Language Understanding,‚Äù**](https://arxiv.org/pdf/1810.04805.pdf) Jacob Devlin, Ming-Wei Chan, Kenton Lee, and Kristina Toutonova, 2018.

    - *Original paper that introduced BERT, authored by Google AI developers* 

- [**‚ÄúContextual Embeddings: When are They Worth It?‚Äù**](https://www.aclweb.org/anthology/2020.acl-main.236/) Simran Arora, Avner May, Jian Zhang, Christopher R√©, 2020. 

-  [**‚ÄúDistilBERT,**](https://arxiv.org/abs/1910.01108)[ **a distilled version of BERT: smaller, faster, cheaper and lighter.‚Äù**](https://arxiv.org/abs/1910.01108) Victor Sanh, Lysandre Debut, Julien Chaumond, Thomas Wolf, 2020. 

    - *Helpful for teaching students how to use BERT-like models without extensive computational resources*

-  [**‚ÄúA**](https://www.mitpressjournals.org/doi/full/10.1162/tacl_a_00349#)[ **Primer in BERTology: What We Know About How BERT Works**](https://www.mitpressjournals.org/doi/full/10.1162/tacl_a_00349#)[,‚Äù](https://www.mitpressjournals.org/doi/full/10.1162/tacl_a_00349#) Anna Rogers, Olga Kovaleva and Anna Rumshisky, 2020.

    - *A survey of 150+ studies of BERT that explores what BERT* *‚Äúknows‚Äù* *and how it might be improved. Very technical and invested in model architecture* 

# Tutorials & Primers

- [**‚ÄúThe Illustrated BERT,  ELMo, and Co.**](http://jalammar.github.io/illustrated-bert/)[ ](http://jalammar.github.io/illustrated-bert/)[**(How**](http://jalammar.github.io/illustrated-bert/)[ **NLP Cracked Transfer Learning),‚Äù**](http://jalammar.github.io/illustrated-bert/) Jay Alammar, December 2018.

    - *Helpful but very technical for a humanities audience* 

# Risks & Ethical Concerns

- [‚ÄúOn the Dangers of Stochastic Parrots: Can Language Models Be Too Big? ü¶ú,‚Äù](https://dl.acm.org/doi/10.1145/3442188.3445922) Emily M. Bender, Timnit Gebru, Angelina McMillan-Major, Shmargaret Shmitchell
    - *This paper discusses the risks and ethical concerns of large language models like BERT, including biased and poorly documented training data as well as financial and environmental costs*

- [**‚ÄúExtracting Data From Large Language Models,‚Äù**](https://arxiv.org/pdf/2012.07805.pdf) Nicholas Carlini, et al, December 2020.

- [**‚ÄúPrivacy Considerations in Large Language Models‚Äù**](https://ai.googleblog.com/2020/12/privacy-considerations-in-large.html) (Blog post), Nicholas Carlini, December 2020.

# Applied Humanities

-  [**‚ÄúDo**](https://tedunderwood.com/2019/07/15/do-humanists-need-bert/10)[ **Humanists Need BERT?, ‚Äú**](https://tedunderwood.com/2019/07/15/do-humanists-need-bert/10) Ted Underwood, July 2019. 

    - *Overview of BERT and an assessment of its usefulness when applied to sentiment analysis of movie reviews and genre classification of books* 

-  [**‚ÄúLiterary**](https://doi.org/10.18653/v1/P19-1353)[ **Event Detection,‚Äù**](https://doi.org/10.18653/v1/P19-1353) Matthew Sims, Jong Ho Park, and David Bamman, 2019.

- [**‚ÄúAn Annotated Dataset of Coreference in English Literature,‚Äù**](https://arxiv.org/abs/1912.01140) David Bamman, Olivia Lewke, and Anya Mansoor.

-  [**‚ÄúLatin**](https://arxiv.org/abs/2009.10053)[ **BERT: A Contextual Language Model for Classical Philology**](https://arxiv.org/abs/2009.10053)[**,**](https://arxiv.org/abs/2009.10053)[**‚Äù**](https://arxiv.org/abs/2009.10053) David Bamman and Patrick Burns.

- [**MacBERTh**](https://www.universiteitleiden.nl/en/news/2020/06/a-whole-new-computational-world)[ ](https://www.universiteitleiden.nl/en/news/2020/06/a-whole-new-computational-world)[**(BERT**](https://www.universiteitleiden.nl/en/news/2020/06/a-whole-new-computational-world)[ **for Early Modern English)**](https://www.universiteitleiden.nl/en/news/2020/06/a-whole-new-computational-world)**,** Lauren Fonteyn.

- [**Unsupervised Domain Adaptation of Contextualized Embeddings for Labeling**](https://www.aclweb.org/anthology/D19-1433/)**.** Xiaochuang Han, Jacob Eisenstein, 2019. 

    - *Domain adaptive fine-tuning on Early Modern English and Twitter*

- [**What about Grammar? Using BERT Embeddings to Explore Functional-Semantic Shifts of Semi-Lexical and Grammatical Constructions**](http://ceur-ws.org/Vol-2723/short15.pdf)**.** Lauren Fonteyn, 2020. 

# Critical Humanities

- [**‚ÄúPlaying With Unicorns:** ](http://www.digitalhumanities.org/dhq/vol/14/4/000533/000533.html)[**AI Dungeon**](http://www.digitalhumanities.org/dhq/vol/14/4/000533/000533.html)[ **and Citizen NLP,‚Äù**](http://www.digitalhumanities.org/dhq/vol/14/4/000533/000533.html)Minh Hua and Rita Raley, *Digital Humanities Quarterly*, 2020.

# Tools

-  [**Easy-Bert**](https://github.com/robrua/easy-bert)**,** Rob Rua

    - *Simple API for BERT* 

-  [**Bert-as-Service**](https://github.com/hanxiao/bert-as-service)**,** Han Xiao

    - *Using BERT as a sentence encoder* 

- [**HuggingFace transformers library**](https://huggingface.co/transformers/index.html)

    - [**HuggingFace BERT model**](https://huggingface.co/transformers/model_doc/bert.html)

- [**AllenNLP Demo**](https://demo.allennlp.org/reading-comprehension) 

# Educational Resources

- [**‚ÄúUsing BERT for next sentence prediction,‚Äù**](https://github.com/sinykin/QTM-340/blob/master/notebooks/class21-BERT-next-sentence-inclass-ds.ipynb) Ted Underwood, adapted and used in Dan Sinykin‚Äôs Emory course ‚ÄúPractical Approaches to Data Science with Text,‚Äù 2020.